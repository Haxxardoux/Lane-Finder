{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import mlflow\n",
    "from time import time\n",
    "import hiddenlayer as HL\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# i made all of these !\n",
    "from models import Backbone as Model\n",
    "from video_loader import vidSet\n",
    "from utils import Params, count_parameters\n",
    "from loss import ConstrastiveLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_path = 'C:\\\\Users\\\\turbo\\\\Python projects\\\\Lane finder\\\\data\\\\videos\\\\test'\n",
    "\n",
    "path_list = []\n",
    "for (dirpath, _, filenames) in os.walk(videos_path):\n",
    "    for filename in filenames:\n",
    "        path_list.append(os.path.abspath(os.path.join(videos_path, filename)))\n",
    "\n",
    "# takes a list of file paths to .mp4s and returns a dataloader ov the frames\n",
    "vidset_train = vidSet(path_list[:2])\n",
    "\n",
    "# we want a class for our parameters because it is wayyyy easier to log them this way \n",
    "args = Params(1, 1, 0.00005)\n",
    "\n",
    "vidloader_train = DataLoader(vidset_train, batch_size=args.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the file path where the logs will be stored. this should be a global reference since many different scripts will reference it from different directories\n",
    "mlflow.tracking.set_tracking_uri('file:\\\\Users\\\\turbo\\\\Python projects\\\\Lane finder\\\\Logs')\n",
    "\n",
    "# a new experiment will be created if one by that name does not already exists\n",
    "mlflow.set_experiment('Constrastive loss unsupervised')\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "def train(model, train_data, loss, optimizer):\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        t0 = time()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch_idx, img_tensor in enumerate(train_data):\n",
    "\n",
    "            # select FOUR images total, two pos two neg. the forward pass size is 1 gb for the overparameterized model so it is important\n",
    "            # to pick a combination that fits into memory. \n",
    "            \n",
    "            # Select 2 positives, or the first 2 frames\n",
    "            positives_tensor = img_tensor[:2].to(device)\n",
    "            \n",
    "            # this I THINK selects the last two images in a sequence, loss does not count observations of the same class, so it is ok that they are similar\n",
    "            # however it may be slower to learn.\n",
    "            negatives_tensor = img_tensor[(len(img_tensor)-2):].to(device)\n",
    "\n",
    "            # print('shape of two positives array: ', positives_tensor.shape)\n",
    "            # print('shape of two negatives array: ', negatives_tensor.shape)\n",
    "\n",
    "            # set parameter gradients to zero \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: compute the output\n",
    "            positive_latent_tensor = model(positives_tensor)\n",
    "            negative_latent_tensor = model(negatives_tensor)\n",
    "\n",
    "            # Computation of the cost J\n",
    "            cost = loss(positive_latent_tensor, negative_latent_tensor)  \n",
    "\n",
    "            # Backward propagation\n",
    "            cost.backward()  # <= compute the gradients\n",
    "\n",
    "            # Update parameters (weights and biais)\n",
    "            optimizer.step()\n",
    "\n",
    "            # hardcoded batch size :( compute the train loss \n",
    "\n",
    "            train_loss += cost.item()\n",
    "        t1 = time()\n",
    "\n",
    "        \n",
    "        ret = {'Train Loss':train_loss, 'Epoch time':t1-t0}\n",
    "        yield ret.items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from models import Backbone as Model\n",
    "\n",
    "model = Model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "\n",
    "run_name = 'One video'\n",
    "with mlflow.start_run(run_name = run_name) as run:\n",
    "    for key, value in vars(args).items():\n",
    "        mlflow.log_param(key, value)\n",
    "\n",
    "    mlflow.log_param('Parameters', count_parameters(model))\n",
    "\n",
    "    for epoch, items in enumerate(train(model, vidloader_train, SupConLoss(), optimizer)):\n",
    "        for key, value in items:\n",
    "            print(key, value)\n",
    "            mlflow.log_metric(key, value, epoch)\n",
    "\n",
    "    torch.save({\n",
    "        'model':model.state_dict(),\n",
    "        'optimizer':optimizer.state_dict(),\n",
    "        }, 'run_stats.pyt')\n",
    "    mlflow.log_artifact('run_stats.pyt')\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # save an architecture diagram\n",
    "    HL.transforms.Fold(\"Conv > BatchNorm > Relu\", \"ConvBnRelu\"),\n",
    "    HL.build_graph(model, torch.zeros([args.batch_size, 3, 288, 512]).to(device)).save('architecture', format='png')\n",
    "    mlflow.log_artifact('architecture.png')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}